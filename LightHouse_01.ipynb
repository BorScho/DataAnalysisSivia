{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Example from the Book p.29 ff)\n",
    "\n",
    "A lighthouse is somewhere off a piece of straight coastline at a position $\\alpha$ (alpha) along the shore and a distance <br>\n",
    "$\\beta$ (beta) out at sea. It emits a series of short highly collimated flashes at random intervals and hence at random azimuths, $\\theta$. <br>\n",
    "These pulses are intercepted on the coast by photo-detectors that record only the fact that a flash has occurred, but not <br>\n",
    "the angle from which it came. <br>\n",
    "N flashes have so far been recorded at positions $\\{x_k\\}$. Where is the lighthouse?â€™\n",
    "\n",
    "<img src=\"pictures01/lightHouseGeometry.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a uniform distribution, $ prob(\\theta | \\{x_k\\}, \\alpha, \\beta, I) = \\frac{1}{\\pi}$ for $\\theta$ between $[- \\pi/2 , + \\pi/2]$, we have a Cauchy-Distribution for the $x_k$ :\n",
    "\n",
    "$$\n",
    "prob(x_k | \\alpha, \\beta, I) = \\frac{\\beta}{\\pi [\\beta^2 + (x_k - \\alpha)^2]}\n",
    "$$\n",
    "\n",
    "Using Bayes' theorem and assuming independence of the measurements, the log-prob function L is given by:\n",
    "\n",
    "$$\n",
    "  L = log(prob(  \\alpha, \\beta | \\{x_k\\} , I) = const + \\sum^N_{k=1}{(log(\\beta) - log(\\beta^2 + (x_k - \\alpha)^2))}\n",
    "$$\n",
    "\n",
    "Our best estimate for $\\alpha$ and $\\beta$ are the values that maximize the log-prob function L.\n",
    "We can take the partial derivatives of L for $\\alpha$ and $\\beta$ and set them equal to zero and solve the equations numerically. Or we use gradient descent to find the minimum of -L, i.e. the maximum of L.\n",
    "Thus we end up with a non-liniear optimization problem / non-linear root finding problem that we will have to solve numerically:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha} = \\sum^N_{k=1}{\\frac{2 (x_k - \\alpha)}{\\beta^2 + (x_k - \\alpha)^2}} = 0 \\\\\n",
    "\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\beta} = \\frac{N}{\\beta} - \\sum^N_{k=1}{\\frac{2 \\beta}{\\beta^2 + (x_k - \\alpha)^2}} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.29331231,  2.16376235,  5.67368268,  2.76885175, -0.01804232,\n",
       "         3.91458902,  1.25565105,  8.31993907, -0.06938472,  0.41978214]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = 1       # on shore position of Lighthouse\n",
    "b = 2       # off shore position of Lighthouse\n",
    "N = 10      # number of datapoints /measurements to generate\n",
    "\n",
    "def generate_x_ks(a, b, N=10):\n",
    "    thetas = np.random.uniform(low= - np.pi * 0.5, high= np.pi * 0.5, size=(1,N))\n",
    "    x_ks = a + b * np.tan(thetas)\n",
    "    return x_ks\n",
    "\n",
    "# test:\n",
    "generate_x_ks(a, b, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.581228290834572"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function that gives back the partial derivation of L for a (as a function):\n",
    "def grad_L_a(x_ks):\n",
    "    def grad(a, b):\n",
    "        diffs = x_ks - a\n",
    "        numerator = diffs\n",
    "        denominator = b**2 + diffs**2\n",
    "        fractions = numerator/denominator\n",
    "        return 2 * np.sum(fractions)  \n",
    "    return grad\n",
    "\n",
    "# test:\n",
    "a = 1\n",
    "b = 2\n",
    "x_ks = generate_x_ks(a,b)\n",
    "\n",
    "dLa = grad_L_a(x_ks)\n",
    "dLa(5, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8212757772981432"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function that gives back the partial derivation of L for b (as a function):\n",
    "def grad_L_b(x_ks):\n",
    "    def grad(a, b):\n",
    "        N = np.size(x_ks)\n",
    "        diffs = x_ks - a\n",
    "        numerator = b\n",
    "        denominator = b**2 + diffs**2\n",
    "        fractions = numerator/denominator\n",
    "        return (N/b) - 2 * np.sum(fractions)  \n",
    "    return grad\n",
    "\n",
    "# test:\n",
    "a = 1\n",
    "b = 2\n",
    "x_ks = generate_x_ks(a,b)\n",
    "dLb = grad_L_b(x_ks)\n",
    "dLb(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.673254727807697, 2.1554659549453996)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement gradient descent to find the maximum of L, i.e. the minimum of -L:\n",
    "\n",
    "def gradient_descent(x, y, x_ks, learning_rate=0.01, N_iterations=1000):\n",
    "    # y is the strarting-point for the off-shore distance - hence allways positive:\n",
    "    if y < 0 or y==0:\n",
    "        return\n",
    "    dL_da = grad_L_a(x_ks)\n",
    "    dL_db = grad_L_b(x_ks)\n",
    "    for _ in np.arange(N_iterations):\n",
    "        x_old = x\n",
    "        x = x + learning_rate * dL_da(x, y) # plus learningrate * ... is correct in this case, because we find the minimum of -L\n",
    "        y = y + learning_rate * dL_db(x_old, y)\n",
    "    return x,y\n",
    "\n",
    "# test:\n",
    "a = 11\n",
    "b = 2\n",
    "x_ks = generate_x_ks(a, b, 100)\n",
    "gradient_descent(43, 7, x_ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additonal things to do:\n",
    "- plot graphs of $prob(\\alpha | \\{x_k\\}, I)$ and $prob(\\beta | \\{x_k\\}, I)$\n",
    "- add (Gaussian) noise to the problem - either in $\\theta$ or in $x_k$\n",
    "- calculate error-bars for $\\alpha$ and $\\beta$ and the correlation-matrix\n",
    "- solve the problem by calculating the zeros of the partial derivatives (i.e. a different algorithm)\n",
    "- plot a sequence of graphs showing the successive approximation of the estimated location and the real location of the light-house. Add 90% prob-contours to the plot\n",
    "- make the sequence of plots an animation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaPytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
